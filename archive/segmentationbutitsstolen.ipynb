{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import elementpath\n",
    "from xml.etree import ElementTree\n",
    "import manga109api\n",
    "from os import listdir\n",
    "from numpy import zeros, asarray, expand_dims, mean\n",
    "from numpy import asarray\n",
    "from mrcnn.utils import Dataset, extract_bboxes, compute_ap\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.visualize import display_instances\n",
    "from mrcnn.model import MaskRCNN, load_image_gt, mold_image\n",
    "import matplotlib.pyplot as pyplot\n",
    "from matplotlib.patches import Rectangle, Arrow\n",
    "import math\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "root_dir = \"D:\\\\python_code\\\\Project-Fugu-Manga-Translator\\\\datasets\\\\Manga109\\\\Manga109_released_2021_12_30\\\\\"\n",
    "p = manga109api.Parser(root_dir=root_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file content already exists.\n"
     ]
    }
   ],
   "source": [
    "%cd ../datasets/Manga109/\n",
    "%mkdir content"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\ARMS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\ARMS already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AisazuNihaIrarenai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AisazuNihaIrarenai already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AkkeraKanjinchou\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AkkeraKanjinchou already exists.\n",
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Akuhamu already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Akuhamu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AosugiruHaru already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AosugiruHaru\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AppareKappore already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AppareKappore\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Arisa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Arisa already exists.\n",
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BEMADER_P already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BEMADER_P\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BakuretsuKungFuGirl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BakuretsuKungFuGirl already exists.\n",
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Belmondo already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Belmondo\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BokuHaSitatakaKun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BokuHaSitatakaKun already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BurariTessenTorimonocho\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BurariTessenTorimonocho already exists.\n",
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\ByebyeC-BOY already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\ByebyeC-BOY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Count3DeKimeteAgeru already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Count3DeKimeteAgeru\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\DollGun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\DollGun already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Donburakokko\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Donburakokko already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\DualJustice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\DualJustice already exists.\n",
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\EienNoWith already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\EienNoWith\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\EvaLady already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\EvaLady\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\EverydayOsakanaChan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\EverydayOsakanaChan already exists.\n",
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\GOOD_KISS_Ver2 already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\GOOD_KISS_Ver2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\GakuenNoise already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\GakuenNoise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\GarakutayaManta already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\GarakutayaManta\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\GinNoChimera\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\GinNoChimera already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Hamlet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Hamlet already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HanzaiKousyouninMinegishiEitarou\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HanzaiKousyouninMinegishiEitarou already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HaruichibanNoFukukoro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HaruichibanNoFukukoro already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HarukaRefrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HarukaRefrain already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HealingPlanet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HealingPlanet already exists.\n",
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HeiseiJimen already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HeiseiJimen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HighschoolKimengumi_vol01 already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HighschoolKimengumi_vol01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HighschoolKimengumi_vol20 already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HighschoolKimengumi_vol20\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HinagikuKenzan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HinagikuKenzan already exists.\n",
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HisokaReturns already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\HisokaReturns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\JangiriPonpon already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\JangiriPonpon\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\JijiBabaFight\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Joouari\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Jyovolley\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\KarappoHighschool\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\KimiHaBokuNoTaiyouDa\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\KoukouNoHitotachi\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\KuroidoGanka\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\KyokugenCyclone\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\LancelotFullThrottle\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\LoveHina_vol01\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\LoveHina_vol14\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MAD_STONE\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MadouTaiga\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MagicStarGakuin\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MagicianLoad\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MariaSamaNihaNaisyo\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MayaNoAkaiKutsu\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MemorySeijin\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MeteoSanStrikeDesu\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MiraiSan\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MisutenaideDaisy\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MoeruOnisan_vol01\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MoeruOnisan_vol19\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MomoyamaHaikagura\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MukoukizuNoChonbo\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\MutekiBoukenSyakuma\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Nekodama\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\NichijouSoup\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Ningyoushi\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\OL_Lunch\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\OhWareraRettouSeitokai\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\PLANET7\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\ParaisoRoad\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\PikaruGenkiDesu\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\PlatinumJungle\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\PrayerHaNemurenai\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\PrismHeart\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\PsychoStaff\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Raphael\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\ReveryEarth\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\RinToSiteSippuNoNaka\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\RisingGirl\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Saisoku\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\SaladDays_vol01\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\SaladDays_vol18\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\SamayoeruSyonenNiJunaiWo\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\SeisinkiVulnus\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\ShimatteIkouze_vol01\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\ShimatteIkouze_vol26\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\SonokiDeABC\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\SyabondamaKieta\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\TaiyouNiSmash\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\TapkunNoTanteisitsu\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\TasogareTsushin\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\TennenSenshiG\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\TensiNoHaneToAkumaNoShippo\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\TetsuSan\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\That'sIzumiko\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\TotteokiNoABC\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\ToutaMairimasu\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\TouyouKidan\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\TsubasaNoKioku\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\UchiNoNyan'sDiary\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\UchuKigekiM774\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\UltraEleven\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\UnbalanceTokyo\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\WarewareHaOniDearu\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\YamatoNoHane\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\YasasiiAkuma\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\YouchienBoueigumi\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\YoumaKourin\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\YukiNoFuruMachi\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\YumeNoKayoiji\n",
      "D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\YumeiroCooking\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for book in p.books:\n",
    "  tree = ElementTree.parse(root_dir + \"annotations/\" + book + \".xml\")\n",
    "  root = tree.getroot()\n",
    "\n",
    "  %mkdir D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\$book\n",
    "  %cd D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\$book\n",
    "\n",
    "  for page in root.findall(\".//page\"):\n",
    "    new_xml = page\n",
    "    b_xml = ElementTree.tostring(new_xml)\n",
    "    with open(\"new_\" + book + str(page.attrib[\"index\"]) + \".xml\", \"wb\") as f:\n",
    "      f.write(b_xml)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Compress-Archive -Path D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AkkeraKanjinchouD:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AkkeraKanjinchou.zip\n",
      "^C\n",
      "Compress-Archive -Path D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AkuhamuD:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Akuhamu.zip\n",
      "^C\n",
      "Compress-Archive -Path D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AosugiruHaruD:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AosugiruHaru.zip\n",
      "^C\n",
      "Compress-Archive -Path D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AppareKapporeD:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\AppareKappore.zip\n",
      "^C\n",
      "Compress-Archive -Path D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\ArisaD:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Arisa.zip\n",
      "^C\n",
      "Compress-Archive -Path D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BEMADER_PD:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BEMADER_P.zip\n",
      "^C\n",
      "Compress-Archive -Path D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BakuretsuKungFuGirlD:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BakuretsuKungFuGirl.zip\n",
      "^C\n",
      "Compress-Archive -Path D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BelmondoD:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\Belmondo.zip\n",
      "^C\n",
      "Compress-Archive -Path D:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BokuHaSitatakaKunD:\\python_code\\Project-Fugu-Manga-Translator\\datasets\\Manga109\\content\\BokuHaSitatakaKun.zip\n"
     ]
    }
   ],
   "source": [
    "for book in p.books:\n",
    "  string = \"Compress-Archive -Path D:\\\\python_code\\\\Project-Fugu-Manga-Translator\\\\datasets\\\\Manga109\\\\content\\\\\" + book + \" D:\\\\python_code\\\\Project-Fugu-Manga-Translator\\\\datasets\\\\Manga109\\\\content\\\\\" + book +\".zip\"\n",
    "  print(string)\n",
    "  !powershell -command $string"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class MangaDataset(Dataset):\n",
    "  def load_dataset(self, is_train=True):\n",
    "    # self.add_class(\"dataset\", 1, \"face\")\n",
    "    self.add_class(\"dataset\", 2, \"text\")\n",
    "    self.add_class(\"dataset\", 3, \"frame\")\n",
    "\n",
    "    last_image_id = 0\n",
    "\n",
    "    for book in sorted(p.books):\n",
    "      images_dir = root_dir + \"images/\" + book + \"/\"\n",
    "      annotations_dir = \"D:\\\\python_code\\\\Project-Fugu-Manga-Translator\\\\datasets\\\\Manga109\\\\content/\" + book + \"/\"\n",
    "\n",
    "      for img in sorted(listdir(images_dir)):\n",
    "        og_image_id = int(img[:-4])\n",
    "        image_id = int(img[:-4]) + last_image_id\n",
    "\n",
    "        tree = ElementTree.parse(annotations_dir + \"new_\" + book + str(og_image_id) + \".xml\")\n",
    "        root = tree.getroot()\n",
    "        faces = []\n",
    "        texts = []\n",
    "        frames = []\n",
    "\n",
    "        for face in root.findall(\".//face\"):\n",
    "          faces.append(face)\n",
    "\n",
    "        for text in root.findall(\".//text\"):\n",
    "          texts.append(text)\n",
    "\n",
    "        for frame in root.findall(\".//frame\"):\n",
    "          frames.append(frame)\n",
    "\n",
    "        if (not faces) or (not texts) or (not frames):\n",
    "          continue\n",
    "\n",
    "        if is_train and og_image_id >= 50:\n",
    "          continue\n",
    "\n",
    "        if not is_train and og_image_id < 50:\n",
    "          continue\n",
    "\n",
    "        img_path = images_dir + img\n",
    "        ann_path = annotations_dir + \"new_\" + book + str(og_image_id) + \".xml\"\n",
    "\n",
    "        self.add_image(\"dataset\", image_id=image_id, path=img_path, annotation=ann_path, class_ids=[0, 1, 2, 3])\n",
    "\n",
    "      last_image_id = image_id + 1\n",
    "\n",
    "\n",
    "  def extract_boxes(self, filename):\n",
    "    tree = ElementTree.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    boxes = []\n",
    "\n",
    "    for box in root.findall(\".//face\"):\n",
    "      continue\n",
    "      att = box.attrib\n",
    "      xmin = att[\"xmin\"]\n",
    "      ymin = att[\"ymin\"]\n",
    "      xmax = att[\"xmax\"]\n",
    "      ymax = att[\"ymax\"]\n",
    "      coors = [xmin, ymin, xmax, ymax, \"face\"]\n",
    "      boxes.append(coors)\n",
    "\n",
    "    for box in root.findall(\".//text\"):\n",
    "      att = box.attrib\n",
    "      xmin = att[\"xmin\"]\n",
    "      ymin = att[\"ymin\"]\n",
    "      xmax = att[\"xmax\"]\n",
    "      ymax = att[\"ymax\"]\n",
    "      coors = [xmin, ymin, xmax, ymax, \"text\"]\n",
    "      boxes.append(coors)\n",
    "\n",
    "    for box in root.findall(\".//frame\"):\n",
    "      att = box.attrib\n",
    "      xmin = att[\"xmin\"]\n",
    "      ymin = att[\"ymin\"]\n",
    "      xmax = att[\"xmax\"]\n",
    "      ymax = att[\"ymax\"]\n",
    "      coors = [xmin, ymin, xmax, ymax, \"frame\"]\n",
    "      boxes.append(coors)\n",
    "\n",
    "    page_att = root.attrib\n",
    "    width = int(page_att[\"width\"])\n",
    "    height = int(page_att[\"height\"])\n",
    "\n",
    "    return boxes, width, height\n",
    "\n",
    "\n",
    "  def load_mask(self, image_id):\n",
    "    info = self.image_info[image_id]\n",
    "    path = info[\"annotation\"]\n",
    "    boxes, w, h = self.extract_boxes(path)\n",
    "\n",
    "    masks = zeros([h, w, len(boxes)], dtype=\"uint8\")\n",
    "\n",
    "    class_ids = []\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "      box = boxes[i]\n",
    "      row_s, row_e = box[1], box[3]\n",
    "      col_s, col_e = box[0], box[2]\n",
    "\n",
    "      if box[4] == \"face\":\n",
    "        continue\n",
    "        masks[int(row_s):int(row_e), int(col_s):int(col_e), i] = 1\n",
    "        class_ids.append(self.class_names.index(\"face\"))\n",
    "\n",
    "      elif box[4] == \"text\":\n",
    "        masks[int(row_s):int(row_e), int(col_s):int(col_e), i] = 2\n",
    "        class_ids.append(self.class_names.index(\"text\"))\n",
    "\n",
    "      elif box[4] == \"frame\":\n",
    "        masks[int(row_s):int(row_e), int(col_s):int(col_e), i] = 3\n",
    "        class_ids.append(self.class_names.index(\"frame\"))\n",
    "\n",
    "    return masks, asarray(class_ids, dtype=\"int32\")\n",
    "\n",
    "\n",
    "  def image_reference(self, image_id):\n",
    "    info = self.image_info[image_id]\n",
    "    return info[\"path\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5048\n"
     ]
    }
   ],
   "source": [
    "train_set = MangaDataset()\n",
    "train_set.load_dataset(is_train=True)\n",
    "train_set.prepare()\n",
    "print(\"Train: %d\" % len(train_set.image_ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 4855\n"
     ]
    }
   ],
   "source": [
    "# test/val set\n",
    "test_set = MangaDataset()\n",
    "test_set.load_dataset(is_train=False)\n",
    "test_set.prepare()\n",
    "print(\"Test: %d\" % len(test_set.image_ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1170, 1654, 3)\n",
      "(1170, 1654, 29)\n"
     ]
    }
   ],
   "source": [
    "# load an image and mask\n",
    "image_id = 1\n",
    "image = test_set.load_image(image_id)\n",
    "print(image.shape)\n",
    "\n",
    "mask, class_ids = test_set.load_mask(image_id)\n",
    "print(mask.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MangaConfig(Config):\n",
    "  NAME = \"manga\"\n",
    "  NUM_CLASSES = 1 + 2\n",
    "  STEPS_PER_EPOCH = 1000\n",
    "\n",
    "  TRAIN_ROIS_PER_IMAGE = 128\n",
    "  IMAGES_PER_GPU = 1\n",
    "\n",
    "  GPU_COUNT = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                15\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           manga\n",
      "NUM_CLASSES                    3\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           128\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = MangaConfig()\n",
    "config.display()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "\n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: model\\manga_cfg20220212T1846\\mask_rcnn_manga_cfg_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "rpn_model              (Functional)\n",
      "anchors                (ConstLayer)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\Project-Fugu-Manga-Translator\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\Project-Fugu-Manga-Translator\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_2:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\Project-Fugu-Manga-Translator\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_1:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_5:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_1:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\Project-Fugu-Manga-Translator\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_8:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\Project-Fugu-Manga-Translator\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_3:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_11:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\Project-Fugu-Manga-Translator\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_2:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\Project-Fugu-Manga-Translator\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_1:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_5:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_1:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\Project-Fugu-Manga-Translator\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_8:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\Project-Fugu-Manga-Translator\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_3:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_11:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\Project-Fugu-Manga-Translator\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_1_grad/Reshape_1:0\", shape=(6000,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_1_grad/Reshape:0\", shape=(6000, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_1_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - ETA: 0s - batch: 499.5000 - size: 1.0000 - loss: 2.7304 - rpn_class_loss: 0.3813 - rpn_bbox_loss: 0.9385 - mrcnn_class_loss: 0.4386 - mrcnn_bbox_loss: 0.5267 - mrcnn_mask_loss: 0.4455"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\Project-Fugu-Manga-Translator\\lib\\site-packages\\keras\\engine\\training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1644s 2s/step - batch: 499.5000 - size: 1.0000 - loss: 2.7304 - rpn_class_loss: 0.3813 - rpn_bbox_loss: 0.9385 - mrcnn_class_loss: 0.4386 - mrcnn_bbox_loss: 0.5267 - mrcnn_mask_loss: 0.4455 - val_loss: 1.9944 - val_rpn_class_loss: 0.1749 - val_rpn_bbox_loss: 0.6400 - val_mrcnn_class_loss: 0.4296 - val_mrcnn_bbox_loss: 0.3675 - val_mrcnn_mask_loss: 0.3825\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 1610s 2s/step - batch: 499.5000 - size: 1.0000 - loss: 1.8024 - rpn_class_loss: 0.1565 - rpn_bbox_loss: 0.5562 - mrcnn_class_loss: 0.4088 - mrcnn_bbox_loss: 0.3333 - mrcnn_mask_loss: 0.3476 - val_loss: 1.5656 - val_rpn_class_loss: 0.1434 - val_rpn_bbox_loss: 0.5106 - val_mrcnn_class_loss: 0.3317 - val_mrcnn_bbox_loss: 0.2786 - val_mrcnn_mask_loss: 0.3013\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 1587s 2s/step - batch: 499.5000 - size: 1.0000 - loss: 1.5146 - rpn_class_loss: 0.1184 - rpn_bbox_loss: 0.4727 - mrcnn_class_loss: 0.3584 - mrcnn_bbox_loss: 0.2680 - mrcnn_mask_loss: 0.2970 - val_loss: 1.3590 - val_rpn_class_loss: 0.1036 - val_rpn_bbox_loss: 0.4469 - val_mrcnn_class_loss: 0.2956 - val_mrcnn_bbox_loss: 0.2377 - val_mrcnn_mask_loss: 0.2752\n"
     ]
    }
   ],
   "source": [
    "model = MaskRCNN(mode=\"training\", model_dir=\"model\", config=config)\n",
    "\n",
    "model.load_weights(\"D:\\python_code\\Project-Fugu-Manga-Translator\\model-weights\\mask_rcnn_coco.h5\",\n",
    "                   by_name=True,\n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=3, layers=\"all\")\n",
    "\n",
    "# config.LEARNING_RATE = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"D:\\python_code\\Project-Fugu-Manga-Translator\\ocr\\mrcnn\\model.py\", line 403, in call  *\n        image_shape = parse_image_meta_graph(image_meta)['image_shape'][0]\n\n    TypeError: tf__batch_pack_graph() missing 2 required positional arguments: 'counts' and 'num_rows'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_21976/2500891832.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;31m# Recreate the model in inference mode\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m model = MaskRCNN(mode=\"inference\",\n\u001B[0m\u001B[0;32m     13\u001B[0m                           \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minference_config\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m                           model_dir=\"model\")\n",
      "\u001B[1;32mD:\\python_code\\Project-Fugu-Manga-Translator\\ocr\\mrcnn\\model.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, mode, config, model_dir)\u001B[0m\n\u001B[0;32m   1833\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_dir\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel_dir\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1834\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_log_dir\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1835\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuild\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1836\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1837\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mbuild\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\python_code\\Project-Fugu-Manga-Translator\\ocr\\mrcnn\\model.py\u001B[0m in \u001B[0;36mbuild\u001B[1;34m(self, mode, config)\u001B[0m\n\u001B[0;32m   2046\u001B[0m             \u001B[1;31m# Proposal classifier and BBox regressor heads\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2047\u001B[0m             \u001B[0mmrcnn_class_logits\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmrcnn_class\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmrcnn_bbox\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2048\u001B[1;33m                 fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta,\n\u001B[0m\u001B[0;32m   2049\u001B[0m                                      \u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPOOL_SIZE\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mNUM_CLASSES\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2050\u001B[0m                                      \u001B[0mtrain_bn\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTRAIN_BN\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\python_code\\Project-Fugu-Manga-Translator\\ocr\\mrcnn\\model.py\u001B[0m in \u001B[0;36mfpn_classifier_graph\u001B[1;34m(rois, feature_maps, image_meta, pool_size, num_classes, train_bn, fc_layers_size)\u001B[0m\n\u001B[0;32m    950\u001B[0m     \u001B[1;31m# ROI Pooling\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    951\u001B[0m     \u001B[1;31m# Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 952\u001B[1;33m     x = PyramidROIAlign([pool_size, pool_size],\n\u001B[0m\u001B[0;32m    953\u001B[0m                         name=\"roi_align_classifier\")([rois, image_meta] + feature_maps)\n\u001B[0;32m    954\u001B[0m     \u001B[1;31m# Two 1024 FC layers (implemented with Conv2D for consistency)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Project-Fugu-Manga-Translator\\lib\\site-packages\\keras\\engine\\base_layer_v1.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    763\u001B[0m               with autocast_variable.enable_auto_cast_variables(\n\u001B[0;32m    764\u001B[0m                   self._compute_dtype_object):\n\u001B[1;32m--> 765\u001B[1;33m                 \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcast_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    766\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    767\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOperatorNotAllowedInGraphError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Project-Fugu-Manga-Translator\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    690\u001B[0m       \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint:disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    691\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'ag_error_metadata'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 692\u001B[1;33m           \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mag_error_metadata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    693\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    694\u001B[0m           \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: in user code:\n\n    File \"D:\\python_code\\Project-Fugu-Manga-Translator\\ocr\\mrcnn\\model.py\", line 403, in call  *\n        image_shape = parse_image_meta_graph(image_meta)['image_shape'][0]\n\n    TypeError: tf__batch_pack_graph() missing 2 required positional arguments: 'counts' and 'num_rows'\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(Config):\n",
    "    NAME = \"manga\"\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    TRAIN_ROIS_PER_IMAGE = 128\n",
    "    NUM_CLASSES = 1 + 2\n",
    "    DETECTION_MAX_INSTANCES = 30\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = MaskRCNN(mode=\"inference\",\n",
    "                          config=inference_config,\n",
    "                          model_dir=\"model\")\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mrcnn.model import log\n",
    "from mrcnn import visualize\n",
    "import random\n",
    "# Test on a random image\n",
    "image_id = 1\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(test_set, inference_config, image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id,\n",
    "                            train_set.class_names, figsize=(8, 8))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'],\n",
    "                            train_set.class_names, r['scores'], ax=get_ax())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "cfg=InferenceConfig()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def evaluate_model(dataset, model, cfg):\n",
    "  APs = []\n",
    "  for image_id in dataset.image_ids:\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id)\n",
    "    scaled_image = mold_image(image, cfg)\n",
    "    sample = expand_dims(scaled_image, 0)\n",
    "    yhat = model.detect(sample, verbose=0)\n",
    "    r = yhat[0]\n",
    "\n",
    "    # change IoU threshold\n",
    "    AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r[\"masks\"], iou_threshold=0.5)\n",
    "    APs.append(AP)\n",
    "\n",
    "  mAP = mean(APs)\n",
    "  return mAP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_image_gt() got an unexpected keyword argument 'use_mini_mask'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16796/1055306738.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrain_mAP\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mevaluate_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_set\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcfg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Train mAP: %.3f\"\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mtrain_mAP\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16796/604789936.py\u001B[0m in \u001B[0;36mevaluate_model\u001B[1;34m(dataset, model, cfg)\u001B[0m\n\u001B[0;32m      2\u001B[0m   \u001B[0mAPs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m   \u001B[1;32mfor\u001B[0m \u001B[0mimage_id\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage_ids\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[0mimage\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimage_meta\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgt_class_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgt_bbox\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgt_mask\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload_image_gt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcfg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimage_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0muse_mini_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m     \u001B[0mscaled_image\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmold_image\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcfg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0msample\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mexpand_dims\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mscaled_image\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: load_image_gt() got an unexpected keyword argument 'use_mini_mask'"
     ]
    }
   ],
   "source": [
    "train_mAP = evaluate_model(train_set, model, cfg)\n",
    "print(\"Train mAP: %.3f\" % train_mAP)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}